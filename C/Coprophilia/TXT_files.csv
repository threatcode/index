63577-0.txt - Project Gutenberg,https://www.gutenberg.org/files/63577/63577-0.txt
banned_words.txt - GitHub,https://github.com/quixio/tutorial-code/blob/main/chat-with-sentiment-exercise/Sentiment%2520analysis/banned_words.txt
human.txt - UNM CS,https://www.cs.unm.edu/~jeffk/tom-skype/dlist-5.0/human.txt
Bad Words - GitHub,https://www.xbond.org/nsfw_words.txt
wordLadder_dictionary.txt - UTRGV Faculty Web,https://faculty.utrgv.edu/zhixiang.chen/cs3333/3333/dic/wordLadder_dictionary.txt
swears.txt - GitHub,https://raw.githubusercontent.com/etylermoss/swears/master/swears.txt
a aardvark aardvarks aardwolf ab abac abaca abacterial abactinal,https://www.khoury.northeastern.edu/home/vkp/3500-fl13/A5/lexicographically_ordered.txt
PigeonholeLive_ChatRestricted...,https://1843972.fs1.hubspotusercontent-na1.net/hubfs/1843972/PigeonholeLive_ChatRestrictedWordsFilterList_Updated.txt
143wi22_section14_dictionary.txt - Washington,https://courses.cs.washington.edu/courses/cse143/22au/sections/143wi22_section14_dictionary.txt
sowpods.txt - MIT,http://web.mit.edu/jesstess/www/sowpods.txt
wordlist.txt - SA Health,https://www.sahealth.sa.gov.au/wps/wcm/connect/cb61702d-e873-4baa-96c9-326ceb577341/wordlist.txt%3FMOD%3DAJPERES%26CONVERT_TO%3Durl%26CACHEID%3DROOTWORKSPACE-cb61702d-e873-4baa-96c9-326ceb577341-o0CcC11
puzzle1000c.txt - FTP Directory Listing,ftp://ftp.cs.princeton.edu/pub/cs226/puzzle/puzzle1000c.txt
wordlist-d.txt - FTP Directory Listing,ftp://ftp.cs.princeton.edu/pub/cs226/puzzle/wordlist-d.txt
agitates fireproofed chariest tuneful Julliard's dustman,https://homepages.math.uic.edu/~leon/mcs360-f07/projects/3-4/dictionary-large-rand.txt
words.txt (big),https://ranger.uta.edu/~alex/courses/1310/data/words.txt
scowl_utf-8.txt - Cornell Computer Science,https://www.cs.cornell.edu/courses/cs1110/2014sp/labs/lab06/scowl_utf-8.txt
unixWordList.txt - MSU CSE,http://www.cse.msu.edu/~cse231/PracticeOfComputingUsingPython/05_ListsTuples/Anagrams/unixWordList.txt
robots.txt - Getty Images,https://www.gettyimages.com/robots.txt
